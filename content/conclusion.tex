\chapter{Conclusion} 
\label{cha:conclusion}

The idea of this work emerged with the benchmark of the On Target (OT). Through the similarity distribution plots, the team noticed that some studies had multiple modes in the holdout set distributions, leading to the hypothesis of multiple profiles in the portfolios. Based on this scenario we used a proof of concept to validate a pre-clustering strategy, where we cluster leads by using firmographics data in order to improve the performance of the OT.

We started this thesis by discussing the context of recommender systems and their relevance for business today. Next, we introduced the concepts of clustering and principal component analysis, which were applied in the upcoming chapters.

After that, we introduced some terminology of the On Target and its benchmark, along with the explanation of how the leads are scored through block diagrams. Then, we tackled the clustering procedures. Concerning the choice of the algorithm and number of clusters we decided to adopt a "manual" approach. The clustering strategy and pairing chosen were, respectively, "\fullNameClusterStrategyA{}" (\nameClusterStrategyA{}) and "\fullNameClusterPairingA{}" (\nameClusterPairingA{}). From both of these, two experiments were designed to test the clustering methods: "\fullNameExperimentI{}" (\nameExperimentI{}) and "\fullNameExperimentII{}" (\nameExperimentII{}).
We analysed their results seeing that the former had a negative gain of approximately $-10\%$ and the latter had a sight improvement of $2\%$. We also, further investigate their results by examining the similarity distribution plots of the studies of the groups former based on the lift gain tables. In this examination we investigated the correlation between the lift and the similarity plot, and other studies' cases that did not yield performance improvement as expected. Finally, we repeated experiment \nameExperimentII{} with other clustering algorithms and verified that none of them surpassed the improvement of the "manual" approach.

Outlined all the steps of this work and its outcomes we conclude that it is not worth to pursue, for now, with the idea of clustering with firmographics data before running the OT. The overall improvement of $2\%$ on the lift is not enough to prioritize the further development of this work on the On Target Product Roadmap at this moment.

\section{Takeaways}

In spite of the unsatisfactory result in the performance attained, this work brought some insights for the team.

\textbf{Focus on the value first}. Clustering the data manually, saved a lot of time and resources from the company. It would take too much from the team to develop a solution that would cluster the data in an automated manner. The value is the impact on the performance of the recommender system, clustering is a way to achieve that. The use of a "Proof of Concept" was an opportunity to quickly test this idea before putting the necessary effort to put it on production;

\textbf{Better understanding about the problem}. We noticed that some of the studies have distinct clusters in their portfolios while others were the case of a single cluster with outlier companies. Moreover, there were studies with a same magnitude of portfolio and market size while others had an unbalanced configuration. This variability of configurations of a study adds a layer of complexity on the OT. It is not a simple model that fulfils the requirement of consistent recommendations for all of the verticals and branches that Neoway addresses. Knowing more about the data in this problem empowers the team to deliver better solutions;

\textbf{Choice of metrics}. The benchmark already brought up the discussion of new metrics for the OT. This work, reinforced that when we discussed in Section \ref{ch:worth-ment} about the study that had a positive lift gain while the sets in the similarity distribution shifted to lower values. This result shows that the lift by itself is not sufficient to evaluate a study.

\section*{Future work}

To become a production-ready solution, the suggested future work is:

\begin{itemize}
    \item to further develop the clustering module. The manual solutions still need to be automated. One possible way is to test the Gaussian Mixture with the tuning of its hyper parameters. Since, as seen in Chapter \ref{ch:other-clustering-algos}, it got the best lift gain from the three algorithms;
    \item to add more features or to change the context of them. The addition of more features or testing different feature contexts is a valid experiment for this work. 
    \item to include suitable metrics. We discussed the limits of the lift on checking the consistency of the study in Chapter \ref{ch:worth-ment}. To better evaluate the studies a new metric that measures the consistency of the output from the OT is necessary;
    \item to get user feedback. To be able to really understand if the proposal solution improved the OT's recommendations, it is necessary to get feedback from the user.
\end{itemize}
