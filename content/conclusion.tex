\chapter{Conclusion} 
\label{cha:conclusion}

The idea of this work emerged after the creation of a benchmark due to an update in the On Target product, which is a recommender system at Neoway that its main purpose is to generate high quality leads for the customers of the company. With the benchmark the team noticed that some studies had multiple high densities areas in the holdout set distributions leading to the hypothesis of multiple profiles in the portfolios. Based on this scenario we used a proof of concept where we would cluster leads by using firmographics data to improve performance.

We started this thesis by discussing the context of recommender systems and their relevance for business today. Next, we introduced the concepts of clustering and principal component analysis, which were applied in the upcoming chapters.

After that, we introduced some terminology of the OT and its benchmark, along with the explanation of how the leads are scored through block diagrams. Then, we tackled the clustering procedures. Concerning the choice of the algorithm and number of clusters we decided to adopt a "manual" approach of solving these problems. The clustering strategy and pairing chosen were \nameClusterStrategyA{} and \nameClusterPairingA{}, respectively. From both of these, two experiments were designed to test the clustering methods: \nameExperimentI{} and \nameExperimentII{}.
We analysed their results seeing that the former had a poor performance of approximately $-10\%$ and the latter had a sightly improvement of $2\%$. We also, further investigate their results by examining the similarity distribution plots of the studies of the groups former based on the lift gain. In this examination we investigated the correlation between the lift and the similarity plot, and other studies' cases that did not yield performance improvement as expected. Finally, we repeated experiment \nameExperimentII{} with other clustering algorithms and verified that none of them surpassed the improvement of the "manual" approach.

Outlined all the steps of this work and its outcomes we conclude that \textbf{it is not worth to pursue with the idea} of clustering with firmographics data before running the OT. We judged that the value we would get from the amount of work remaining to do is minimal. For instance, the clustering module is not optimal yet, when comparing to the result achieved by the manual approach. Hence, more work in testing new algorithms or adjusting parameters of a specific one would be required. All of this for an improvement in performance by $2\%$. On top of that, this is the mean lift gain amongst all studies, that is, some studies had a decrease over $10\%$ in the lift while others had an increase. So there is the possibility of hurting the user with the recommendations. If the result had more consistency, for example, over $10\%$ mean improvement with, let us say, less then 3 studies with negative results then the further improvement of the PoC  would be included in the development pipeline of the On Target's at Neoway. 

\section{Takeaways}

In spite of the unsatisfactory result in the performance attained, this work brought some insights for the team:
\begin{itemize}
    \item the practical approaches, like clustering the data manually, saved a lot of time and resources from the company. It would take too much from the team to develop a solution that clustered the data automatically without even knowing the value that it could bring. The PoC is an opportunity to quickly test an idea before putting the necessary effort to put it on production;
    \item this was an opportunity to learn more about the problem. We noticed that some of the studies have distinct clusters on their portfolios while others were the case of a single cluster with outlier companies. This correlates well with the way of how Neoway's users use the OT. Some of them segment their portfolio based on their sales strategy, thus the data is already "clustered" by our client. This variability adds a layer of complexity to the design of the system. Moreover, Neoway address more than 20 verticals in the market. And the statistics of the data varies amongst them (i.e a retailer is different from a bank), in such way that this is not an easy problem. It is not a simple model that fulfils the requirement of consistent recommendations for all of these verticals. Knowing more about the data in this problem empowers the team to deliver better solutions;
    \item it showed to us that sometimes the simplest solution is the best way to go, as seen in experiment \nameExperimentII{}. This is a crucial factor when we take to account the fact that Neoway is a software company and its products are developed by a team of people. A complex solution hinders the maintainability aspect of the product, since it introduces more states for testing, logging and debugging;
    \item we could test the limits of the OT. The minimum data to a valid run was the beginning of the inspection of how many companies must be in the portfolio and market (and their ratio) for the OT run with reliable results. We inspected some examples where the results were awful when their portfolio size were negligible when comparing to the market;
    \item it enabled us to see that the lift (performance) is not the only relevant metric to evaluate the result of a study. The benchmark already brought up the discussion of new metrics for the OT. This work, reinforced that when we discussed in section \ref{ch:worth-ment} about the study that had a positive lift gain while the sets in the similarity distribution shifted to lower values. This result raises the awareness of the team to improve the metrics and the consistency of the solution;
\end{itemize}

%%% OBS %%%
% Usually there is a future work section, however I stated that it is not worth it to continue with this.

% \section{Future work}
% Run in the production environment
% Improve the GMM algorithms (mess with the hyper parameters)
% Redo number of clusters assignment based on the results of the GMM clustering